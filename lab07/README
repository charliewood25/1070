
In this lab we'll use a kNN classifer to do handwriting recognition.

The objects we're trying to classify are images of handwritten
digits (one digit in each image); given an image, we'd like to assign
the correct label between 0 and 9.  We have several thousand of these
that can be used for training and testing; these come from a well-known
dataset (the so-called MNIST database of handwritten digits; see
http://yann.lecun.com/exdb/mnist/).  You can see sample images
of the digit "5" here, for example:
http://cs.nyu.edu/~roweis/data/mnist_train5.jpg

The training data are contained in train.csv. If you open that
file in your favorite text editor, you'll see that it is a standard
comma-separated values file.  As usual, each row represents one of
the objects we'd like to classify, in this case, an image.  The
first entry in the row is the correct label for the image, a 
value between 0 and 9. The remaining entries in the row represent
grayscale pixel values, ranging from 0 (the background color, white)
to 255 (black).  Each image has been rescaled to 28x28 pixels,
so there are 785 entries in row (one for the label, plus 28*28=784).
There are 5000 rows available for training in the file train.csv,
and a test set with 100 examples in the file test.csv 
(in exactly the same format).

My code the kNN classifier is contained in the file knn.py.
You should open that file and familiarize yourself with it since
you'll need to change some of the parameters (in particular, the
value of "k").

(1) Start by running the classifier with the default settings
(500 training examples and k=1):

$ python knn.py

This will run through the 100 test examples and outputs the
classifier's guess along with the correct answer for each one.
At the end, the results are summarized as a percent accuracy,
followed by what we call a "confusion matrix".  This shows
us exactly which classes (digits) were the hardest to predict.
The rows of the confusion matrix represent the actual, correct
labels, and the columns represent the guessed labels. 

In this first, run which single error (digit x being mistken for digit y)
occurred most frequently?  

Which digits were always labeled perfectly?

Which digits had the largest number of incorrect labelings?
Do these results match your intuition about this problem? 


(2) Now try experimenting with the parameters to see how much you
can improve the accuracy.  Try increasing the amount of training
data by changing the value of "traincount" near the top of the program.
How much does this improve accuracy for a given k value?

Then try varying k and see what happens.


(3) In class I talked about different distance measures that one
can use to compare two vectors.  One family of distances were the
so-called "Lp" distances.  The code as it stands uses the Euclidean 
(L2) distance.  Can you modify the code and try, say, the
L1 (Manhattan) distance?  Or L3?  Do either of these improve 
the accuracy over L2?
