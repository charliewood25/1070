

(1) First, some pencil and paper work.  Compute the entropy of
the following sets; you can do this by hand as long as you know that
log (base 2) of 3 is about 1.585.

   (a) {Y,X,X}
   (b) {Y,X,X,X}
   (c) {X,X,X,Y,Y,Z}
   (d) {W,X,Y,Z}
   (e) {W,W,W,W,X,X,Y,Z}


(2) In class I claimed that text compression algorithms can take
advantage of the non-randomness in natural language (lower entropy)
to achieve better compression results.  Here's an experiment to see
if there's any validity to this. You'll find four text files in this
directory named amm.txt, wbp.txt, hun.txt, and csb.txt.  These are
text selections in the languages Ama (spoken in Papua New Guinea),
Warlpiri (Australia), Hungarian (Hungary), and Kashubian (Poland), 
respectively.

By looking at the letter distributions in these files, I've estimated
estimate the entropy of each language as follows:

amm.txt: 2.702 bits/symbol
wbp.txt: 2.722 bits/symbol
hun.txt: 3.940 bits/symbol
csb.txt: 4.068 bits/symbol

Which is to say that Ama and Warlpiri have relatively small entropy, and
Hungarian and Kashubian have relatively high entropy.  If you open the
text files, you might be able to "see" this lack of randomness - a kind of
repetitiveness or redundancy in the writing system for the first two.
And at a surface level, there are more symbols used in the writing systems
for the latter two, when accents, and special characters are taken into 
account. This contributes to the higher entropy also.

There are a few compression programs you can use from the command line.
For example, you can compress amm.txt using "Bzip2":

$ bzip2 amm.txt

resulting in a file named "amm.txt.bz2".

Uncompress it as follows:

$ bunzip2 amm.txt.bz2

Similarly,

$ gzip amm.txt

to produce a compressed file "amm.txt.gz"

which can be uncompressed with

$ gunzip amm.txt.gz

Or, you can use "zip", which requires you to give the name of the zip file:

$ zip amm.zip amm.txt

Uncompress with:

$ unzip amm.zip

Apply these three compression algorithms to each of the four files.
Compare the ratio of the file size before and after compression.
You can use "ls -l" at the command line to see the size of all
file in the current directory.

Which algorithm gives the best compression?
Which languages can be compressed the best?
Does this fit with your intuition about entropy?

(3) In this exercise you'll learn the key step in the construction
of a decision tree classifier, and will apply it to the "Weather" 
spreadsheet we looked at in class.

In lecture on Monday, I talked about a simple baseline algorithm
that involved looking at a single column (feature) in the spreadsheet.
For each possible feature value, you look at the training examples
with that feature value, and the corresponding distribution of labels
for those examples.  Our algorithm simply chooses the label that appears
most frequently.  

The question to ask now is, "which column is the best one to use?"
One approach to this was to see how many time this algorithm gives
the right answer on the training data.  Entropy gives us way to compute
a more mathematically sophisticated answer.

Consider the "Outlook" column in our spreadsheet as an example.
Outlook == overcast occurs four times, with distribution {Y,Y,Y,Y}.
Outlook == rainy occurs five times, with distribution {Y,Y,Y,N,N}.
Outlook == sunny occurs five times, with distribution {Y,Y,N,N,N}.

(a) First, compute the entropy of each of these distributions. A
calculator will help here!   Call these three entropies O, R, and S.
(b) The {Y,Y,Y,Y} for overcast makes us happy but it would have been
better if it had occurred more than 4/14 times in training. Which 
is to say that we want to weight these entropies by how many
elements are in each distribution.  Compute the weighted average of
the entropies as follows:

(4/14)*O + (5/14)*R + (5/14)*S

This number is a measure of the effectiveness of the Outlook
feature in classifying examples.  

(c) Repeat this same process for the Temp, Humidity, and Windy 
columns to get a weighted entropy for each.  The column with
the lowest weighted entropy is the "best" feature to use for
this classification problem.

Make sure you've worked this out before class on Monday.  I'll
use this calculation as a jumping-off point to describe the rest of
the classification process.
